{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 2DPoissonPINN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRUJwJ7DGxyr",
        "colab_type": "text"
      },
      "source": [
        "Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNvwp6zLFVLm",
        "colab_type": "code",
        "outputId": "41f24d57-640f-4a1a-b8b5-3d203ca78c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs #Github"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'PINNs' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1-wA47XFdLD",
        "colab_type": "code",
        "outputId": "1557a97d-b18d-47e9-cada-72d57062b852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        }
      },
      "source": [
        "#%tensorflow_version 2.x\n",
        "!pip install tensorflow==2.2.0-rc3 pyDOE"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.2.0-rc3 in /usr/local/lib/python3.6/dist-packages (2.2.0rc3)\n",
            "Requirement already satisfied: pyDOE in /usr/local/lib/python3.6/dist-packages (0.3.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (0.3.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.28.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (0.34.2)\n",
            "Requirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (2.2.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (2.2.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.6.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (3.10.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.4.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (3.2.1)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0-rc3) (2.10.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2.21.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.6.0.post3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.7.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.2.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.24.3)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0-rc3) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DALfnaJaFmed",
        "colab_type": "code",
        "outputId": "e238f59c-fd04-4493-eff1-2f2f020c4b1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "print(tf.__version__)\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8al_lFquG4-T",
        "colab_type": "text"
      },
      "source": [
        "Get Data + Graph funcs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3cWSa_BFp_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.io\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from matplotlib import cm\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")\n",
        "\n",
        "sys.path.insert(0, utilsPath)\n",
        "from plotting import newfig, savefig\n",
        "\n",
        "def prep_dataPoisson(path, N_u=None, N_f=None, N_n=None, q=None, ub=None, lb=None, noise=0.0, idx_t_0=None, idx_t_1=None, N_0=None, N_1=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = np.load(\"/content/drive/My Drive/ph253/Project/data(-1,1)square200x200/x(3).npy\") # T x 1\n",
        "    x = np.load(\"/content/drive/My Drive/ph253/Project/data(-1,1)square200x200/y(3).npy\") # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.load(\"/content/drive/My Drive/ph253/Project/data(-1,1)square200x200/convVoltage(4).npy\").T # T x N\n",
        "\n",
        "    if N_n != None and q != None and ub != None and lb != None and idx_t_0 != None and idx_t_1 != None:\n",
        "      dt = t[idx_t_1] - t[idx_t_0]\n",
        "      idx_x = np.random.choice(Exact_u.shape[1], N_n, replace=False) \n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_t_0:idx_t_0+1,idx_x].T\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "        \n",
        "      # Boudanry data\n",
        "      x_1 = np.vstack((lb, ub))\n",
        "      \n",
        "      # Test data\n",
        "      x_star = x\n",
        "      u_star = Exact_u[idx_t_1,:]\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      IRK_weights = np.reshape(tmp[0:q**2+q], (q+1,q))\n",
        "      IRK_times = tmp[q**2+q:]\n",
        "\n",
        "      return x, t, dt, Exact_u, x_0, u_0, x_1, x_star, u_star, IRK_weights, IRK_times\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    if N_0 != None and N_1 != None:\n",
        "      Exact_u = Exact_u.T\n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_0, replace=False)\n",
        "      x_0 = x[idx_x,:]\n",
        "      u_0 = Exact_u[idx_x,idx_t_0][:,None]\n",
        "      u_0 = u_0 + noise*np.std(u_0)*np.random.randn(u_0.shape[0], u_0.shape[1])\n",
        "          \n",
        "      idx_x = np.random.choice(Exact_u.shape[0], N_1, replace=False)\n",
        "      x_1 = x[idx_x,:]\n",
        "      u_1 = Exact_u[idx_x,idx_t_1][:,None]\n",
        "      u_1 = u_1 + noise*np.std(u_1)*np.random.randn(u_1.shape[0], u_1.shape[1])\n",
        "      \n",
        "      dt = np.asscalar(t[idx_t_1] - t[idx_t_0])        \n",
        "      q = int(np.ceil(0.5*np.log(np.finfo(float).eps)/np.log(dt)))\n",
        "\n",
        "      # Load IRK weights\n",
        "      tmp = np.float32(np.loadtxt(os.path.join(utilsPath, \"IRK_weights\", \"Butcher_IRK%d.txt\" % (q)), ndmin = 2))\n",
        "      weights =  np.reshape(tmp[0:q**2+q], (q+1,q))     \n",
        "      IRK_alpha = weights[0:-1,:]\n",
        "      IRK_beta = weights[-1:,:] \n",
        "      return x_0, u_0, x_1, u_1, x, t, dt, q, Exact_u, IRK_alpha, IRK_beta\n",
        "\n",
        "    if N_f == None:\n",
        "      lb = X_star.min(axis=0)\n",
        "      ub = X_star.max(axis=0) \n",
        "      return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, ub, lb\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=-1) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "    # Getting the highest boundary conditions (x=1) \n",
        "    xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "    uu3 = Exact_u[:,-1:]\n",
        "    # Getting the right boundary conditions (t=1) ##added by andreas ->\n",
        "    xx4 = np.hstack((X[-1:,:].T, T[-1:,:].T))\n",
        "    uu4 = Exact_u[-1:,:].T\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2, xx3, xx4])\n",
        "    u_train = np.vstack([uu1, uu2, uu3, uu4])       ## <-\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=10):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    self.model = model\n",
        "    print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t, file=None):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "  from matplotlib import rc\n",
        "  rc(\"text\", usetex=False)\n",
        "\n",
        "  # Creating the figures\n",
        "#  fig, ax = newfig(1.0, 1.1)\n",
        "  fig, ax=plt.subplots(figsize=(7,7))\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap=cm.get_cmap(\"seismic\"), \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "  ax.set_aspect(\"equal\")\n",
        "  fig.savefig(\"2-D Poisson PINN.png\")\n",
        "#  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "#  ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "#  ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "#  ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$y$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(x,y)$', fontsize = 10)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 3)\n",
        "  gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])\n",
        "  ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-1.1,1.1])\n",
        "  ax.set_ylim([-1.1,1.1])    \n",
        "  ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "def plot_inf_disc_results(x_star, idx_t_0, idx_t_1, x_0, u_0, ub, lb, u_1_pred, Exact_u, x, t, file=None):\n",
        "  fig, ax = newfig(1.0, 1.2)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  ####### Row 0: h(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/2 + 0.1, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "  \n",
        "  h = ax.imshow(Exact_u.T, interpolation='nearest', cmap='rainbow', \n",
        "                extent=[t.min(), t.max(), x_star.min(), x_star.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "      \n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  ax.plot(t[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "  \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  leg = ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  \n",
        "  ####### Row 1: h(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/2-0.05, bottom=0.15, left=0.15, right=0.85, wspace=0.5)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[idx_t_0,:], 'b-', linewidth = 2) \n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_0]), fontsize = 10)\n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.8, -0.3), ncol=2, frameon=False)\n",
        "\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x, Exact_u[idx_t_1,:], 'b-', linewidth = 2, label = 'Exact') \n",
        "  ax.plot(x_star, u_1_pred, 'r--', linewidth = 2, label = 'Prediction')      \n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = %.2f$' % (t[idx_t_1]), fontsize = 10)    \n",
        "  ax.set_xlim([lb-0.1, ub+0.1])\n",
        "  \n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.1, -0.3), ncol=2, frameon=False)\n",
        "    \n",
        "  plt.show()\n",
        "\n",
        "  if file != None:\n",
        "    savefig(file)\n",
        "\n",
        "\n",
        "def plot_ide_disc_results(x_star, t_star, idx_t_0, idx_t_1, x_0, u_0, x_1, u_1,\n",
        "  ub, lb, u_1_pred, Exact, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy,\n",
        "  x, t, file=None):  \n",
        "  fig, ax = newfig(1.0, 1.5)\n",
        "  ax.axis('off')\n",
        "  \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3+0.05, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "      \n",
        "  h = ax.imshow(Exact, interpolation='nearest', cmap='rainbow',\n",
        "                extent=[t_star.min(),t_star.max(), lb[0], ub[0]],\n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "  \n",
        "  line = np.linspace(x_star.min(), x_star.max(), 2)[:,None]\n",
        "  ax.plot(t_star[idx_t_0]*np.ones((2,1)), line, 'w-', linewidth = 1.0)\n",
        "  ax.plot(t_star[idx_t_1]*np.ones((2,1)), line, 'w-', linewidth = 1.0)    \n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "  \n",
        "  gs1 = gridspec.GridSpec(1, 2)\n",
        "  gs1.update(top=1-1/3-0.1, bottom=1-2/3, left=0.15, right=0.85, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x_star,Exact[:,idx_t_0][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_0, u_0, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_0], u_0.shape[0]), fontsize = 10)\n",
        "  \n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x_star,Exact[:,idx_t_1][:,None], 'b', linewidth = 2, label = 'Exact')\n",
        "  ax.plot(x_1, u_1, 'rx', linewidth = 2, label = 'Data')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.set_title('$t = %.2f$\\n%d trainng data' % (t_star[idx_t_1], u_1.shape[0]), fontsize = 10)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(-0.3, -0.3), ncol=2, frameon=False)\n",
        "  \n",
        "  gs2 = gridspec.GridSpec(1, 2)\n",
        "  gs2.update(top=1-2/3-0.05, bottom=0, left=0.15, right=0.85, wspace=0.0)\n",
        "  \n",
        "  ax = plt.subplot(gs2[0, 0])\n",
        "  ax.axis('off')\n",
        "  nu = 0.01/np.pi\n",
        "  s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x + %.6f u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & ' % (nu)\n",
        "  s2 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "  s3 = r'Identified PDE (1\\% noise) & '\n",
        "  s4 = r'$u_t + %.3f u u_x + %.6f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "  s5 = r'\\end{tabular}$'\n",
        "  s = s1+s2+s3+s4+s5\n",
        "  ax.text(-0.1,0.2,s)\n",
        "  plt.show()\n",
        "\n",
        "def plot_ide_cont_results(X_star, u_pred, X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t, lambda_1_value, lambda_1_value_noisy, lambda_2_value, lambda_2_value_noisy):\n",
        "    fig, ax = newfig(1.0, 1.4)\n",
        "    ax.axis('off')\n",
        "\n",
        "    U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
        "    \n",
        "    ####### Row 0: u(t,x) ##################    \n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1.0/3.0+0.06, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "    \n",
        "    h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
        "                  extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                  origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "    \n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 2, clip_on = False)\n",
        "    \n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    \n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(1.0, -0.125), ncol=5, frameon=False)\n",
        "    ax.set_title('$u(t,x)$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################    \n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,Exact_u[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')    \n",
        "    ax.set_title('$t = 0.25$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,Exact_u[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "    \n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,Exact_u[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
        "    ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(t,x)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])    \n",
        "    ax.set_title('$t = 0.75$', fontsize = 10)\n",
        "    \n",
        "    ####### Row 3: Identified PDE ##################    \n",
        "    gs2 = gridspec.GridSpec(1, 3)\n",
        "    gs2.update(top=1.0-2.0/3.0, bottom=0, left=0.0, right=1.0, wspace=0.0)\n",
        "    \n",
        "    ax = plt.subplot(gs2[:, :])\n",
        "    ax.axis('off')\n",
        "    s1 = r'$\\begin{tabular}{ |c|c| }  \\hline Correct PDE & $u_t + u u_x - 0.0031831 u_{xx} = 0$ \\\\  \\hline Identified PDE (clean data) & '\n",
        "    s2 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$ \\\\  \\hline ' % (lambda_1_value, lambda_2_value)\n",
        "    s3 = r'Identified PDE (1\\% noise) & '\n",
        "    s4 = r'$u_t + %.5f u u_x - %.7f u_{xx} = 0$  \\\\  \\hline ' % (lambda_1_value_noisy, lambda_2_value_noisy)\n",
        "    s5 = r'\\end{tabular}$'\n",
        "    s = s1+s2+s3+s4+s5\n",
        "    ax.text(0.1,0.1,s)\n",
        "    plt.show()\n",
        "    # savefig('./figures/Burgers_identification')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZo_DqC4GFqh",
        "colab_type": "text"
      },
      "source": [
        "custom_lbfgs.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCYCk9dOFtXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-5\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 1\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPsybZoYGIY3",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87ZAblk0GJTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Data size on the solution u\n",
        "N_u = 50\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "N_f = 10000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 100\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate=0.1,\n",
        "  beta_1=0.99,\n",
        "  epsilon=1e-1)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 2000\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 0.8\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 50\n",
        "nt_config.tolFun = 1.0 * np.finfo(float).eps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLmh1-OcHVbt",
        "colab_type": "text"
      },
      "source": [
        "PINN class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFSM8W_OGnxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, X_f, ub, lb, nu):\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 2.0*(X - lb)/(ub - lb) - 1.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.nu = nu\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Separating the collocation coordinates\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
        "    \n",
        "  # Defining custom loss\n",
        "  def __loss(self, u, u_pred):\n",
        "    f_pred = self.f_model()\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + \\\n",
        "      tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  # The actual PINN\n",
        "  def f_model(self):\n",
        "    # Using the new GradientTape paradigm of TF2.0,\n",
        "    # which keeps track of operations to get the gradient at runtime\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      # Watching the two inputs we’ll need later, x and t\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      # Packing together the inputs\n",
        "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
        "\n",
        "      # Getting the prediction\n",
        "      u = self.u_model(X_f)\n",
        "      # Deriving INSIDE the tape (since we’ll need the x derivative of this later, u_xx)\n",
        "      u_x = tape.gradient(u, self.x_f)\n",
        "      u_t = tape.gradient(u, self.t_f)\n",
        "      \n",
        "    \n",
        "    # Getting the other derivatives\n",
        "    u_xx = tape.gradient(u_x, self.x_f)\n",
        "    u_tt = tape.gradient(u_t, self.t_f)\n",
        "    \n",
        "\n",
        "    # Letting the tape go\n",
        "    del tape\n",
        "\n",
        "    nu = self.get_params(numpy=True)\n",
        "\n",
        "    # Buidling the PINNs\n",
        "    return u_tt + u_xx + (tf.exp(-tf.square(self.t_f))+tf.exp(-tf.square(self.x_f)))\n",
        "\n",
        "  def get_params(self, numpy=False):\n",
        "    return self.nu\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.u_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def summary(self):\n",
        "    return self.u_model.summary()\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs=5000, nt_config=Struct()):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "    \n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "    # tfp.optimizer.lbfgs_minimize(\n",
        "    #   loss_and_flat_grad,\n",
        "    #   initial_position=self.get_weights(),\n",
        "    #   num_correction_pairs=nt_config.nCorrection,\n",
        "    #   max_iterations=nt_config.maxIter,\n",
        "    #   f_relative_tolerance=nt_config.tolFun,\n",
        "    #   tolerance=nt_config.tolFun,\n",
        "    #   parallel_iterations=6)\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    f_star = self.f_model()\n",
        "    return u_star, f_star"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-8v4OleHZzh",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTjr73lzGsOA",
        "colab_type": "code",
        "outputId": "8043f79e-e2e6-476b-c046-12c0db41d882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "# Getting the data\n",
        "path = os.path.join(appDataPath, \"burgers_shock.mat\")\n",
        "x, t, X, T, Exact_u, X_star, u_star, \\\n",
        "  X_u_train, u_train, X_f, ub, lb = prep_dataPoisson(path, N_u, N_f, noise=0.0)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=10)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb, nu=0.01/np.pi)\n",
        "def error():\n",
        "  u_pred, _ = pinn.predict(X_star)\n",
        "  return np.linalg.norm(u_star - u_pred, 2) / np.linalg.norm(u_star, 2)\n",
        "logger.set_error_fn(error)\n",
        "pinn.fit(X_u_train, u_train, tf_epochs, nt_config)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred, f_pred = pinn.predict(X_star)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.2.0-rc3\n",
            "Eager execution: True\n",
            "GPU-accerelated: False\n",
            "\n",
            "Training started\n",
            "================\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda_7 (Lambda)            (None, 2)                 0         \n",
            "_________________________________________________________________\n",
            "dense_72 (Dense)             (None, 20)                60        \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_75 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_78 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_79 (Dense)             (None, 20)                420       \n",
            "_________________________________________________________________\n",
            "dense_80 (Dense)             (None, 1)                 21        \n",
            "=================================================================\n",
            "Total params: 3,021\n",
            "Trainable params: 3,021\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "—— Starting Adam optimization ——\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "tf_epoch =      0  elapsed = 00:00  loss = 2.4761e+00  error = 7.9205e-01  \n",
            "tf_epoch =     10  elapsed = 00:04  loss = 1.0293e+00  error = 7.8062e-01  \n",
            "tf_epoch =     20  elapsed = 00:07  loss = 1.6944e-01  error = 8.2621e-01  \n",
            "tf_epoch =     30  elapsed = 00:11  loss = 2.3701e-01  error = 5.6267e-01  \n",
            "tf_epoch =     40  elapsed = 00:14  loss = 2.2761e-01  error = 6.8413e-01  \n",
            "tf_epoch =     50  elapsed = 00:18  loss = 1.4507e-01  error = 5.4634e-01  \n",
            "tf_epoch =     60  elapsed = 00:21  loss = 1.4666e-01  error = 4.0450e-01  \n",
            "tf_epoch =     70  elapsed = 00:25  loss = 1.3792e-01  error = 7.7047e-01  \n",
            "tf_epoch =     80  elapsed = 00:28  loss = 9.9734e-02  error = 3.5315e-01  \n",
            "tf_epoch =     90  elapsed = 00:32  loss = 5.0350e-02  error = 2.5172e-01  \n",
            "—— Starting LBFGS optimization ——\n",
            "nt_epoch =     10  elapsed = 00:39  loss = 3.3693e-02  error = 2.1616e-01  \n",
            "nt_epoch =     20  elapsed = 00:42  loss = 1.9514e-02  error = 1.8977e-01  \n",
            "nt_epoch =     30  elapsed = 00:46  loss = 1.3815e-02  error = 2.7770e-01  \n",
            "nt_epoch =     40  elapsed = 00:50  loss = 1.1324e-02  error = 2.2175e-01  \n",
            "nt_epoch =     50  elapsed = 00:53  loss = 9.1035e-03  error = 2.0050e-01  \n",
            "nt_epoch =     60  elapsed = 00:57  loss = 7.9268e-03  error = 1.9378e-01  \n",
            "nt_epoch =     70  elapsed = 01:01  loss = 7.3711e-03  error = 2.0411e-01  \n",
            "nt_epoch =     80  elapsed = 01:05  loss = 6.8330e-03  error = 1.9080e-01  \n",
            "nt_epoch =     90  elapsed = 01:09  loss = 6.3779e-03  error = 1.8501e-01  \n",
            "nt_epoch =    100  elapsed = 01:13  loss = 5.9173e-03  error = 1.6985e-01  \n",
            "nt_epoch =    110  elapsed = 01:19  loss = 5.6978e-03  error = 1.7648e-01  \n",
            "nt_epoch =    120  elapsed = 01:22  loss = 5.1043e-03  error = 1.5987e-01  \n",
            "nt_epoch =    130  elapsed = 01:26  loss = 4.5929e-03  error = 1.4699e-01  \n",
            "nt_epoch =    140  elapsed = 01:30  loss = 4.2699e-03  error = 1.3660e-01  \n",
            "nt_epoch =    150  elapsed = 01:33  loss = 3.6256e-03  error = 1.3380e-01  \n",
            "nt_epoch =    160  elapsed = 01:37  loss = 3.4057e-03  error = 1.2655e-01  \n",
            "nt_epoch =    170  elapsed = 01:41  loss = 2.8966e-03  error = 1.1356e-01  \n",
            "nt_epoch =    180  elapsed = 01:44  loss = 2.6366e-03  error = 1.0964e-01  \n",
            "nt_epoch =    190  elapsed = 01:48  loss = 2.4354e-03  error = 1.0605e-01  \n",
            "nt_epoch =    200  elapsed = 01:52  loss = 2.2720e-03  error = 1.0606e-01  \n",
            "nt_epoch =    210  elapsed = 01:56  loss = 2.0090e-03  error = 9.7260e-02  \n",
            "nt_epoch =    220  elapsed = 01:59  loss = 1.9371e-03  error = 9.1530e-02  \n",
            "nt_epoch =    230  elapsed = 02:03  loss = 1.6884e-03  error = 7.9321e-02  \n",
            "nt_epoch =    240  elapsed = 02:07  loss = 1.5579e-03  error = 7.6319e-02  \n",
            "nt_epoch =    250  elapsed = 02:11  loss = 1.4813e-03  error = 7.8324e-02  \n",
            "nt_epoch =    260  elapsed = 02:14  loss = 1.3786e-03  error = 7.2743e-02  \n",
            "nt_epoch =    270  elapsed = 02:18  loss = 1.3107e-03  error = 6.6359e-02  \n",
            "nt_epoch =    280  elapsed = 02:22  loss = 1.2410e-03  error = 6.1955e-02  \n",
            "nt_epoch =    290  elapsed = 02:25  loss = 1.0995e-03  error = 5.3695e-02  \n",
            "nt_epoch =    300  elapsed = 02:29  loss = 9.8911e-04  error = 4.9789e-02  \n",
            "nt_epoch =    310  elapsed = 02:33  loss = 8.6653e-04  error = 5.0753e-02  \n",
            "nt_epoch =    320  elapsed = 02:36  loss = 8.0340e-04  error = 4.3468e-02  \n",
            "nt_epoch =    330  elapsed = 02:40  loss = 7.4616e-04  error = 4.2281e-02  \n",
            "nt_epoch =    340  elapsed = 02:44  loss = 6.5724e-04  error = 4.3251e-02  \n",
            "nt_epoch =    350  elapsed = 02:48  loss = 5.5425e-04  error = 3.5673e-02  \n",
            "nt_epoch =    360  elapsed = 02:51  loss = 4.9727e-04  error = 2.9632e-02  \n",
            "nt_epoch =    370  elapsed = 02:55  loss = 4.4470e-04  error = 2.7329e-02  \n",
            "nt_epoch =    380  elapsed = 02:59  loss = 4.0193e-04  error = 2.5033e-02  \n",
            "nt_epoch =    390  elapsed = 03:02  loss = 3.5442e-04  error = 2.1684e-02  \n",
            "nt_epoch =    400  elapsed = 03:06  loss = 3.2656e-04  error = 2.5597e-02  \n",
            "nt_epoch =    410  elapsed = 03:10  loss = 2.9045e-04  error = 2.5812e-02  \n",
            "nt_epoch =    420  elapsed = 03:14  loss = 2.6765e-04  error = 2.3346e-02  \n",
            "nt_epoch =    430  elapsed = 03:18  loss = 2.4720e-04  error = 2.2310e-02  \n",
            "nt_epoch =    440  elapsed = 03:22  loss = 2.3473e-04  error = 2.2355e-02  \n",
            "nt_epoch =    450  elapsed = 03:26  loss = 2.0211e-04  error = 2.1412e-02  \n",
            "nt_epoch =    460  elapsed = 03:29  loss = 1.8516e-04  error = 2.0900e-02  \n",
            "nt_epoch =    470  elapsed = 03:33  loss = 1.6548e-04  error = 2.1054e-02  \n",
            "nt_epoch =    480  elapsed = 03:37  loss = 1.5332e-04  error = 2.0443e-02  \n",
            "nt_epoch =    490  elapsed = 03:40  loss = 1.4310e-04  error = 2.0208e-02  \n",
            "nt_epoch =    500  elapsed = 03:44  loss = 1.3182e-04  error = 1.8710e-02  \n",
            "nt_epoch =    510  elapsed = 03:48  loss = 1.2915e-04  error = 1.8341e-02  \n",
            "nt_epoch =    520  elapsed = 03:51  loss = 1.2231e-04  error = 1.8894e-02  \n",
            "nt_epoch =    530  elapsed = 03:55  loss = 1.1703e-04  error = 2.0319e-02  \n",
            "nt_epoch =    540  elapsed = 03:59  loss = 1.0481e-04  error = 1.7744e-02  \n",
            "nt_epoch =    550  elapsed = 04:02  loss = 9.6954e-05  error = 1.7091e-02  \n",
            "nt_epoch =    560  elapsed = 04:06  loss = 9.5041e-05  error = 1.7854e-02  \n",
            "nt_epoch =    570  elapsed = 04:10  loss = 8.9285e-05  error = 1.7543e-02  \n",
            "nt_epoch =    580  elapsed = 04:13  loss = 8.0221e-05  error = 1.6278e-02  \n",
            "nt_epoch =    590  elapsed = 04:17  loss = 7.4302e-05  error = 1.5809e-02  \n",
            "nt_epoch =    600  elapsed = 04:21  loss = 6.8275e-05  error = 1.5567e-02  \n",
            "nt_epoch =    610  elapsed = 04:24  loss = 6.6274e-05  error = 1.5389e-02  \n",
            "nt_epoch =    620  elapsed = 04:28  loss = 6.3912e-05  error = 1.4941e-02  \n",
            "nt_epoch =    630  elapsed = 04:32  loss = 6.0027e-05  error = 1.5093e-02  \n",
            "nt_epoch =    640  elapsed = 04:35  loss = 5.6437e-05  error = 1.6222e-02  \n",
            "nt_epoch =    650  elapsed = 04:39  loss = 5.2877e-05  error = 1.5490e-02  \n",
            "nt_epoch =    660  elapsed = 04:43  loss = 4.9103e-05  error = 1.4609e-02  \n",
            "nt_epoch =    670  elapsed = 04:46  loss = 4.5018e-05  error = 1.5033e-02  \n",
            "nt_epoch =    680  elapsed = 04:50  loss = 4.1177e-05  error = 1.4298e-02  \n",
            "nt_epoch =    690  elapsed = 04:54  loss = 3.9062e-05  error = 1.3045e-02  \n",
            "nt_epoch =    700  elapsed = 04:58  loss = 3.7934e-05  error = 1.3389e-02  \n",
            "nt_epoch =    710  elapsed = 05:01  loss = 3.6570e-05  error = 1.3392e-02  \n",
            "nt_epoch =    720  elapsed = 05:05  loss = 3.6027e-05  error = 1.3708e-02  \n",
            "nt_epoch =    730  elapsed = 05:09  loss = 3.5634e-05  error = 1.3702e-02  \n",
            "nt_epoch =    740  elapsed = 05:12  loss = 3.5318e-05  error = 1.3960e-02  \n",
            "nt_epoch =    750  elapsed = 05:16  loss = 3.4858e-05  error = 1.3450e-02  \n",
            "nt_epoch =    760  elapsed = 05:20  loss = 3.4255e-05  error = 1.3883e-02  \n",
            "nt_epoch =    770  elapsed = 05:24  loss = 3.3449e-05  error = 1.2514e-02  \n",
            "nt_epoch =    780  elapsed = 05:27  loss = 3.1340e-05  error = 1.2613e-02  \n",
            "nt_epoch =    790  elapsed = 05:31  loss = 3.0711e-05  error = 1.3010e-02  \n",
            "nt_epoch =    800  elapsed = 05:35  loss = 3.0155e-05  error = 1.3491e-02  \n",
            "nt_epoch =    810  elapsed = 05:38  loss = 2.9853e-05  error = 1.3148e-02  \n",
            "nt_epoch =    820  elapsed = 05:42  loss = 2.9569e-05  error = 1.2986e-02  \n",
            "nt_epoch =    830  elapsed = 05:46  loss = 2.9215e-05  error = 1.2749e-02  \n",
            "nt_epoch =    840  elapsed = 05:49  loss = 2.8959e-05  error = 1.2865e-02  \n",
            "nt_epoch =    850  elapsed = 05:53  loss = 2.8692e-05  error = 1.2528e-02  \n",
            "nt_epoch =    860  elapsed = 05:57  loss = 2.8424e-05  error = 1.2846e-02  \n",
            "nt_epoch =    870  elapsed = 06:01  loss = 2.7835e-05  error = 1.3088e-02  \n",
            "nt_epoch =    880  elapsed = 06:04  loss = 2.7386e-05  error = 1.3017e-02  \n",
            "nt_epoch =    890  elapsed = 06:08  loss = 2.6905e-05  error = 1.2649e-02  \n",
            "nt_epoch =    900  elapsed = 06:12  loss = 2.6342e-05  error = 1.3028e-02  \n",
            "nt_epoch =    910  elapsed = 06:16  loss = 2.5187e-05  error = 1.3038e-02  \n",
            "nt_epoch =    920  elapsed = 06:20  loss = 2.4916e-05  error = 1.3060e-02  \n",
            "nt_epoch =    930  elapsed = 06:23  loss = 2.4755e-05  error = 1.3057e-02  \n",
            "nt_epoch =    940  elapsed = 06:27  loss = 2.4462e-05  error = 1.2766e-02  \n",
            "nt_epoch =    950  elapsed = 06:31  loss = 2.4207e-05  error = 1.2875e-02  \n",
            "nt_epoch =    960  elapsed = 06:34  loss = 2.3816e-05  error = 1.2956e-02  \n",
            "nt_epoch =    970  elapsed = 06:38  loss = 2.3643e-05  error = 1.2950e-02  \n",
            "nt_epoch =    980  elapsed = 06:42  loss = 2.3489e-05  error = 1.2566e-02  \n",
            "nt_epoch =    990  elapsed = 06:45  loss = 2.2985e-05  error = 1.2589e-02  \n",
            "nt_epoch =   1000  elapsed = 06:49  loss = 2.2552e-05  error = 1.2395e-02  \n",
            "nt_epoch =   1010  elapsed = 06:53  loss = 2.1997e-05  error = 1.3081e-02  \n",
            "nt_epoch =   1020  elapsed = 06:56  loss = 2.1648e-05  error = 1.2738e-02  \n",
            "nt_epoch =   1030  elapsed = 07:00  loss = 2.1450e-05  error = 1.2798e-02  \n",
            "nt_epoch =   1040  elapsed = 07:04  loss = 2.1187e-05  error = 1.2760e-02  \n",
            "nt_epoch =   1050  elapsed = 07:07  loss = 2.0857e-05  error = 1.2398e-02  \n",
            "nt_epoch =   1060  elapsed = 07:11  loss = 2.0627e-05  error = 1.2206e-02  \n",
            "nt_epoch =   1070  elapsed = 07:15  loss = 2.0429e-05  error = 1.2294e-02  \n",
            "nt_epoch =   1080  elapsed = 07:18  loss = 2.0085e-05  error = 1.2842e-02  \n",
            "nt_epoch =   1090  elapsed = 07:22  loss = 1.9849e-05  error = 1.2232e-02  \n",
            "nt_epoch =   1100  elapsed = 07:26  loss = 1.9353e-05  error = 1.2211e-02  \n",
            "nt_epoch =   1110  elapsed = 07:29  loss = 1.9038e-05  error = 1.2336e-02  \n",
            "nt_epoch =   1120  elapsed = 07:33  loss = 1.8764e-05  error = 1.2372e-02  \n",
            "nt_epoch =   1130  elapsed = 07:37  loss = 1.8553e-05  error = 1.2120e-02  \n",
            "nt_epoch =   1140  elapsed = 07:40  loss = 1.8421e-05  error = 1.2179e-02  \n",
            "nt_epoch =   1150  elapsed = 07:44  loss = 1.8243e-05  error = 1.2215e-02  \n",
            "nt_epoch =   1160  elapsed = 07:48  loss = 1.8136e-05  error = 1.2216e-02  \n",
            "nt_epoch =   1170  elapsed = 07:51  loss = 1.7960e-05  error = 1.2480e-02  \n",
            "nt_epoch =   1180  elapsed = 07:55  loss = 1.7844e-05  error = 1.2428e-02  \n",
            "nt_epoch =   1190  elapsed = 07:59  loss = 1.7757e-05  error = 1.2589e-02  \n",
            "nt_epoch =   1200  elapsed = 08:02  loss = 1.7640e-05  error = 1.2480e-02  \n",
            "nt_epoch =   1210  elapsed = 08:06  loss = 1.7439e-05  error = 1.2284e-02  \n",
            "nt_epoch =   1220  elapsed = 08:09  loss = 1.7266e-05  error = 1.2393e-02  \n",
            "nt_epoch =   1230  elapsed = 08:13  loss = 1.7012e-05  error = 1.2590e-02  \n",
            "nt_epoch =   1240  elapsed = 08:17  loss = 1.6770e-05  error = 1.2421e-02  \n",
            "nt_epoch =   1250  elapsed = 08:21  loss = 1.6623e-05  error = 1.2466e-02  \n",
            "nt_epoch =   1260  elapsed = 08:24  loss = 1.6542e-05  error = 1.2429e-02  \n",
            "nt_epoch =   1270  elapsed = 08:28  loss = 1.6428e-05  error = 1.2146e-02  \n",
            "nt_epoch =   1280  elapsed = 08:32  loss = 1.6156e-05  error = 1.2184e-02  \n",
            "nt_epoch =   1290  elapsed = 08:36  loss = 1.5987e-05  error = 1.2340e-02  \n",
            "nt_epoch =   1300  elapsed = 08:40  loss = 1.5761e-05  error = 1.2622e-02  \n",
            "nt_epoch =   1310  elapsed = 08:44  loss = 1.5606e-05  error = 1.2535e-02  \n",
            "nt_epoch =   1320  elapsed = 08:47  loss = 1.5442e-05  error = 1.2459e-02  \n",
            "nt_epoch =   1330  elapsed = 08:51  loss = 1.5256e-05  error = 1.2533e-02  \n",
            "nt_epoch =   1340  elapsed = 08:55  loss = 1.5045e-05  error = 1.2730e-02  \n",
            "nt_epoch =   1350  elapsed = 08:58  loss = 1.4694e-05  error = 1.2649e-02  \n",
            "nt_epoch =   1360  elapsed = 09:02  loss = 1.4562e-05  error = 1.2630e-02  \n",
            "nt_epoch =   1370  elapsed = 09:06  loss = 1.4462e-05  error = 1.2896e-02  \n",
            "nt_epoch =   1380  elapsed = 09:09  loss = 1.4387e-05  error = 1.2697e-02  \n",
            "nt_epoch =   1390  elapsed = 09:13  loss = 1.4255e-05  error = 1.2427e-02  \n",
            "nt_epoch =   1400  elapsed = 09:17  loss = 1.4093e-05  error = 1.2286e-02  \n",
            "nt_epoch =   1410  elapsed = 09:20  loss = 1.3972e-05  error = 1.2458e-02  \n",
            "nt_epoch =   1420  elapsed = 09:24  loss = 1.3890e-05  error = 1.2453e-02  \n",
            "==================\n",
            "Training finished (epoch 2100): duration = 09:27  error = 1.2261e-02  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF-tPXAiIYjj",
        "colab_type": "code",
        "outputId": "63658d2b-a4ae-4042-cbd0-3ae50f700c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEam_r13Hb_x",
        "colab_type": "text"
      },
      "source": [
        "Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe-tjzE6GvIp",
        "colab_type": "code",
        "outputId": "e6b94b2d-e6d2-444c-b5db-6d03477151e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train,\n",
        "  Exact_u, X, T, x, t)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVaIzZi0ZOlk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtest"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}